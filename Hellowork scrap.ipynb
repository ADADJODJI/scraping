{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e89d464d-536e-4066-aa6d-8aae84cbe092",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.service import Service as ChromeService \n",
    "from webdriver_manager.chrome import ChromeDriverManager \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys # le button entrer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d52b5542-1fa5-40c5-a38b-3131403695b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting link on page: 100%|██████████| 27/27 [01:02<00:00,  2.33s/it]\n",
      "processing link: 100%|██████████| 540/540 [08:12<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.service import Service as ChromeService \n",
    "from webdriver_manager.chrome import ChromeDriverManager \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys # le button entrer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "url= \"https://www.hellowork.com/fr-fr/\"\n",
    "\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# naviger sur la page\n",
    "driver.get(url)\n",
    "\n",
    "# Sélectionner la barre de recherche\n",
    "search_input = driver.find_element(by=By.NAME,value=\"k\")\n",
    "\n",
    "# element de recherche\n",
    "search_input.send_keys(\"data\")\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "# Button entrer\n",
    "search_input.send_keys(Keys.RETURN)\n",
    "\n",
    "# temps d'attente\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Définir l'expression XPath pour le bouton \"Continuer sans accepter\"\n",
    "continue_button_xpath = '//button[@id=\"hw-cc-notice-continue-without-accepting-btn\"]'\n",
    "\n",
    "continue_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, continue_button_xpath))\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    # Cliquer sur le bouton \"Continuer sans accepter\"\n",
    "continue_button.click()\n",
    "\n",
    "# Définir l'expression XPath pour l'élément <li>\n",
    "\n",
    "xpath_expression = '//li[@class=\"next\"]'\n",
    "\n",
    "# Attendre que l'élément soit cliquable\n",
    "element = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.XPATH, xpath_expression))\n",
    ")\n",
    "\n",
    "# Cliquer sur l'élément\n",
    "element.click()\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "\n",
    "def page_link():\n",
    "\n",
    "#\"\"\"avoir les lins sur une page\"\"\"\n",
    "\n",
    "    # pour stoker les lien\n",
    "    link= []\n",
    "        # Définir l'expression XPath\n",
    "    css_expression = '.offer--content .offer--maininfo h3 a'\n",
    "\n",
    "    # Attendre la présence des éléments avant de les récupérer\n",
    "    job_links = WebDriverWait(driver, 1).until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, css_expression))\n",
    "    )\n",
    "    # Utiliser les liens d'emploi récupérés\n",
    "    for job_link in job_links:\n",
    "        lien_offre = job_link.get_attribute('href')\n",
    "        link.append(lien_offre)\n",
    "    \n",
    "    return link\n",
    "\n",
    "def change_page():\n",
    "    # Définir l'expression XPath pour l'élément <li>\n",
    "\n",
    "    xpath_expression = '//li[@class=\"next\"]'\n",
    "\n",
    "    # Attendre que l'élément soit cliquable\n",
    "    element = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, xpath_expression))\n",
    "    )\n",
    "\n",
    "    # Cliquer sur l'élément\n",
    "    element.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "def get_all_link(nb_page):\n",
    "\n",
    "    link=[]\n",
    "\n",
    "    for _ in tqdm(range(1,nb_page), desc='Getting link on page'):\n",
    "\n",
    "        link.extend(page_link())\n",
    "        # next page\n",
    "        change_page()\n",
    "    \n",
    "    return link\n",
    "\n",
    "# Fermez le navigateur après avoir terminé\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "# Get link data  specify the number of page to get\n",
    "link_data = get_all_link(28)\n",
    "#########################################################################\n",
    "driver.quit()\n",
    "\n",
    "# sauvegarder les lien\n",
    "#df = pd.DataFrame({\n",
    "#    \"Lien\":link_data})\n",
    "\n",
    "#df.to_csv(r'C:\\Users\\33753\\Desktop\\PYLAB\\Algorithm\\hellowork_data1.csv', index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def get_job(url):\n",
    "\n",
    "    # Fetch the HTML content from the URL with headers\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Utilisez la méthode find pour sélectionner l'élément span désiré\n",
    "\n",
    "    # Titre du poste\n",
    "    job_title_span = soup.find('span', {'class': 'tw-block tw-typo-xl sm:tw-typo-3xl tw-mb-2', 'data-cy': 'jobTitle'})\n",
    "    if job_title_span is not None:\n",
    "        job_title = job_title_span.text.strip()\n",
    "    else:\n",
    "        job_title = None\n",
    "\n",
    "    # Nom de l'entreprise\n",
    "    company_name_span = soup.find('span', {'class': 'tw-contents tw-typo-m tw-text-grey'})\n",
    "    if company_name_span is not None:\n",
    "        company_name = company_name_span.text.strip()\n",
    "    else:\n",
    "        company_name = None\n",
    "\n",
    "    # Emplacement et type de poste\n",
    "    spans = soup.find_all('span', {'class': 'tw-inline-flex tw-typo-m tw-text-grey'})\n",
    "    if len(spans) >= 2:\n",
    "        job_type_span = spans[1].text.strip()\n",
    "        location_span = spans[0].text.strip()\n",
    "    else:\n",
    "        job_type_span = None\n",
    "        location_span = None\n",
    "\n",
    "    # Salaire\n",
    "    salary_span = soup.find('li', {'class': 'tw-tag-attractive-s tw-readonly'})\n",
    "    if salary_span is not None:\n",
    "        salary = salary_span.text.strip().replace('\\u202f', '')\n",
    "    else:\n",
    "        salary = None\n",
    "\n",
    "    # Date de publication\n",
    "    date_span = soup.find('span', {'class': 'tw-block tw-typo-xs tw-text-grey tw-mt-3 tw-break-words'})\n",
    "    if date_span is not None:\n",
    "        date_text = date_span.text.strip().split(' ')\n",
    "        date = date_text[2]\n",
    "        ref = date_text[6].split('/')[0]\n",
    "    else:\n",
    "        date = None\n",
    "        ref = None\n",
    "\n",
    "    # Référence de l'annonce\n",
    "    ref_span = soup.find('span', {'class': 'tw-block tw-typo-xs tw-text-grey tw-mt-3 tw-break-words'})\n",
    "    if ref_span is not None:\n",
    "        ref_text = ref_span.text.strip().split(' ')\n",
    "        ref = ref_text[6].split('/')[0]\n",
    "    else:\n",
    "        ref = None\n",
    "\n",
    "    # Trouver l'élément <p> par son nom de classe\n",
    "    paragraph_element = soup.find('p', class_='tw-typo-long-m')\n",
    "\n",
    "    # Extraire le texte de l'élément <p>\n",
    "    paragraph_text = paragraph_element.get_text(strip=True)\n",
    "\n",
    "    # Extraire le texte de l'élément <p>\n",
    "    if paragraph_element is not None:\n",
    "        paragraph_text = paragraph_element.get_text(strip=True)\n",
    "    else:\n",
    "        paragraph_text = None\n",
    "    \n",
    "    return job_title, company_name,salary,location_span, paragraph_text,date,job_type_span, ref\n",
    "        \n",
    "\n",
    "\n",
    "def get_infos(data_link):\n",
    "    # Initialize lists to store individual pieces of information\n",
    "    job_titles = []\n",
    "    company_names = []\n",
    "    job_types = []\n",
    "    salaries = []\n",
    "    locations = []\n",
    "    job_descriptions = []\n",
    "    Date=[]\n",
    "    ID= []\n",
    "\n",
    "    # Iterate through each URL in the provided list\n",
    "    for url in tqdm(data_link, desc='processing link'):\n",
    "        # Call the get_job function to extract information from the current URL\n",
    "        infos = get_job(url)\n",
    "\n",
    "        # Append the extracted information to the respective lists\n",
    "        job_titles.append(infos[0])\n",
    "        company_names.append(infos[1])\n",
    "        job_types.append(infos[6])\n",
    "        salaries.append(infos[2])\n",
    "        locations.append(infos[3])\n",
    "        job_descriptions.append(infos[4])\n",
    "        Date.append(infos[5])\n",
    "        ID.append(infos[7])\n",
    "        \n",
    "\n",
    "    # Create a dictionary with the collected information\n",
    "    data = {\n",
    "        'Job ID':ID,\n",
    "        'Job Title': job_titles,\n",
    "        'Company Name': company_names,\n",
    "        'Job Type': job_types,\n",
    "        'Date':Date,\n",
    "        'Salary': salaries,\n",
    "        'Location': locations,\n",
    "        'Job Description': job_descriptions,\n",
    "        'lien':link_data\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary into a DataFrame using pandas\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Return the DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "df= get_infos(link_data)\n",
    "\n",
    "df.to_csv(r'C:\\Users\\komla\\Desktop\\Projet_Scraping\\hellowork_data7.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eb54051-771f-4598-8a58-db196229c72c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting link on page:  66%|██████▋   | 99/149 [04:08<02:16,  2.74s/it]"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: disconnected: not connected to DevTools\n  (failed to check if window was closed: disconnected: not connected to DevTools)\n  (Session info: chrome=120.0.6099.225)\nStacktrace:\n\tGetHandleVerifier [0x00BB6EE3+174339]\n\t(No symbol) [0x00AE0A51]\n\t(No symbol) [0x007F6FF6]\n\t(No symbol) [0x007E957F]\n\t(No symbol) [0x007E92A2]\n\t(No symbol) [0x007F8B70]\n\t(No symbol) [0x00859F88]\n\t(No symbol) [0x00846DA6]\n\t(No symbol) [0x00821034]\n\t(No symbol) [0x00821F8D]\n\tGetHandleVerifier [0x00C54B1C+820540]\n\tsqlite3_dbdata_init [0x00D153EE+653550]\n\tsqlite3_dbdata_init [0x00D14E09+652041]\n\tsqlite3_dbdata_init [0x00D097CC+605388]\n\tsqlite3_dbdata_init [0x00D15D9B+656027]\n\t(No symbol) [0x00AEFE6C]\n\t(No symbol) [0x00AE83B8]\n\t(No symbol) [0x00AE84DD]\n\t(No symbol) [0x00AD5818]\n\tBaseThreadInitThunk [0x76246839+25]\n\tRtlGetFullPathName_UEx [0x7799906F+1215]\n\tRtlGetFullPathName_UEx [0x7799903D+1165]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 98\u001b[0m\n\u001b[0;32m     94\u001b[0m         change_page()\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m link\n\u001b[1;32m---> 98\u001b[0m link_data \u001b[38;5;241m=\u001b[39m get_all_link(\u001b[38;5;241m150\u001b[39m)\n\u001b[0;32m    100\u001b[0m driver\u001b[38;5;241m.\u001b[39mquit()\n\u001b[0;32m    102\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    104\u001b[0m }\n",
      "Cell \u001b[1;32mIn[1], line 92\u001b[0m, in \u001b[0;36mget_all_link\u001b[1;34m(nb_page)\u001b[0m\n\u001b[0;32m     88\u001b[0m link\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,nb_page), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGetting link on page\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 92\u001b[0m     link\u001b[38;5;241m.\u001b[39mextend(page_link())\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m# Move to the next page\u001b[39;00m\n\u001b[0;32m     94\u001b[0m     change_page()\n",
      "Cell \u001b[1;32mIn[1], line 65\u001b[0m, in \u001b[0;36mpage_link\u001b[1;34m()\u001b[0m\n\u001b[0;32m     62\u001b[0m css_expression \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.offer--content .offer--maininfo h3 a\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Wait for elements to be present before retrieving them\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m job_links \u001b[38;5;241m=\u001b[39m WebDriverWait(driver, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39muntil(\n\u001b[0;32m     66\u001b[0m     EC\u001b[38;5;241m.\u001b[39mpresence_of_all_elements_located((By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, css_expression))\n\u001b[0;32m     67\u001b[0m )\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m job_link \u001b[38;5;129;01min\u001b[39;00m job_links:\n\u001b[0;32m     70\u001b[0m     offer_link \u001b[38;5;241m=\u001b[39m job_link\u001b[38;5;241m.\u001b[39mget_attribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:96\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m         value \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_driver)\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value:\n\u001b[0;32m     98\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\support\\expected_conditions.py:191\u001b[0m, in \u001b[0;36mpresence_of_all_elements_located.<locals>._predicate\u001b[1;34m(driver)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predicate\u001b[39m(driver: WebDriverOrWebElement):\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m driver\u001b[38;5;241m.\u001b[39mfind_elements(\u001b[38;5;241m*\u001b[39mlocator)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:772\u001b[0m, in \u001b[0;36mWebDriver.find_elements\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    768\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    770\u001b[0m \u001b[38;5;66;03m# Return empty list if driver returns null\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;66;03m# See https://github.com/SeleniumHQ/selenium/issues/4555\u001b[39;00m\n\u001b[1;32m--> 772\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mFIND_ELEMENTS, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing\u001b[39m\u001b[38;5;124m\"\u001b[39m: by, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: value})[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:348\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    346\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[0;32m    349\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: disconnected: not connected to DevTools\n  (failed to check if window was closed: disconnected: not connected to DevTools)\n  (Session info: chrome=120.0.6099.225)\nStacktrace:\n\tGetHandleVerifier [0x00BB6EE3+174339]\n\t(No symbol) [0x00AE0A51]\n\t(No symbol) [0x007F6FF6]\n\t(No symbol) [0x007E957F]\n\t(No symbol) [0x007E92A2]\n\t(No symbol) [0x007F8B70]\n\t(No symbol) [0x00859F88]\n\t(No symbol) [0x00846DA6]\n\t(No symbol) [0x00821034]\n\t(No symbol) [0x00821F8D]\n\tGetHandleVerifier [0x00C54B1C+820540]\n\tsqlite3_dbdata_init [0x00D153EE+653550]\n\tsqlite3_dbdata_init [0x00D14E09+652041]\n\tsqlite3_dbdata_init [0x00D097CC+605388]\n\tsqlite3_dbdata_init [0x00D15D9B+656027]\n\t(No symbol) [0x00AEFE6C]\n\t(No symbol) [0x00AE83B8]\n\t(No symbol) [0x00AE84DD]\n\t(No symbol) [0x00AD5818]\n\tBaseThreadInitThunk [0x76246839+25]\n\tRtlGetFullPathName_UEx [0x7799906F+1215]\n\tRtlGetFullPathName_UEx [0x7799903D+1165]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.service import Service as ChromeService \n",
    "from webdriver_manager.chrome import ChromeDriverManager \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url= \"https://www.hellowork.com/fr-fr/\"\n",
    "\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to the page\n",
    "driver.get(url)\n",
    "\n",
    "# Select the search bar\n",
    "search_input = driver.find_element(by=By.NAME,value=\"k\")\n",
    "\n",
    "# Search for a keyword\n",
    "search_input.send_keys(\"data\")\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "# Press Enter\n",
    "search_input.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for some time\n",
    "time.sleep(5)\n",
    "\n",
    "# Define XPath expression for the \"Continue without accepting\" button\n",
    "continue_button_xpath = '//button[@id=\"hw-cc-notice-continue-without-accepting-btn\"]'\n",
    "\n",
    "continue_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, continue_button_xpath))\n",
    "    )\n",
    "\n",
    "# Click on the \"Continue without accepting\" button\n",
    "continue_button.click()\n",
    "\n",
    "# Define XPath expression for the \"li\" element\n",
    "xpath_expression = '//li[@class=\"next\"]'\n",
    "\n",
    "# Wait for the element to be clickable\n",
    "element = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.XPATH, xpath_expression))\n",
    ")\n",
    "\n",
    "# Click on the element\n",
    "element.click()\n",
    "time.sleep(5)\n",
    "\n",
    "def page_link():\n",
    "\n",
    "    # To store the links\n",
    "    link= []\n",
    "\n",
    "    css_expression = '.offer--content .offer--maininfo h3 a'\n",
    "\n",
    "    # Wait for elements to be present before retrieving them\n",
    "    job_links = WebDriverWait(driver, 1).until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, css_expression))\n",
    "    )\n",
    "    \n",
    "    for job_link in job_links:\n",
    "        offer_link = job_link.get_attribute('href')\n",
    "        link.append(offer_link)\n",
    "    \n",
    "    return link\n",
    "\n",
    "def change_page():\n",
    "\n",
    "    xpath_expression = '//li[@class=\"next\"]'\n",
    "\n",
    "    element = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, xpath_expression))\n",
    "    )\n",
    "\n",
    "    element.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "def get_all_link(nb_page):\n",
    "\n",
    "    link=[]\n",
    "\n",
    "    for _ in tqdm(range(1,nb_page), desc='Getting link on page'):\n",
    "\n",
    "        link.extend(page_link())\n",
    "        # Move to the next page\n",
    "        change_page()\n",
    "    \n",
    "    return link\n",
    "\n",
    "link_data = get_all_link(150)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def get_job(url):\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Use the find method to select the desired span element\n",
    "\n",
    "    # Job title\n",
    "    job_title_span = soup.find('span', {'class': 'tw-block tw-typo-xl sm:tw-typo-3xl tw-mb-2', 'data-cy': 'jobTitle'})\n",
    "    if job_title_span is not None:\n",
    "        job_title = job_title_span.text.strip()\n",
    "    else:\n",
    "        job_title = None\n",
    "\n",
    "    # Company name\n",
    "    company_name_span = soup.find('span', {'class': 'tw-contents tw-typo-m tw-text-grey'})\n",
    "    if company_name_span is not None:\n",
    "        company_name = company_name_span.text.strip()\n",
    "    else:\n",
    "        company_name = None\n",
    "\n",
    "    # Location and job type\n",
    "    spans = soup.find_all('span', {'class': 'tw-inline-flex tw-typo-m tw-text-grey'})\n",
    "    if len(spans) >= 2:\n",
    "        job_type_span = spans[1].text.strip()\n",
    "        location_span = spans[0].text.strip()\n",
    "    else:\n",
    "        job_type_span = None\n",
    "        location_span = None\n",
    "\n",
    "    # Salary\n",
    "    salary_span = soup.find('li', {'class': 'tw-tag-attractive-s tw-readonly'})\n",
    "    if salary_span is not None:\n",
    "        salary = salary_span.text.strip().replace('\\u202f', '')\n",
    "    else:\n",
    "        salary = None\n",
    "\n",
    "    # Publication date\n",
    "    date_span = soup.find('span', {'class': 'tw-block tw-typo-xs tw-text-grey tw-mt-3 tw-break-words'})\n",
    "    if date_span is not None:\n",
    "        date_text = date_span.text.strip().split(' ')\n",
    "        date = date_text[2]\n",
    "        ref = date_text[6].split('/')[0]\n",
    "    else:\n",
    "        date = None\n",
    "        ref = None\n",
    "\n",
    "    # Advertisement reference\n",
    "    ref_span = soup.find('span', {'class': 'tw-block tw-typo-xs tw-text-grey tw-mt-3 tw-break-words'})\n",
    "    if ref_span is not None:\n",
    "        ref_text = ref_span.text.strip().split(' ')\n",
    "        ref = ref_text[6].split('/')[0]\n",
    "    else:\n",
    "        ref = None\n",
    "\n",
    "    # Find the <p> element by its class name\n",
    "    paragraph_element = soup.find('p', class_='tw-typo-long-m')\n",
    "\n",
    "    # Extract text from the <p> element\n",
    "    paragraph_text = paragraph_element.get_text(strip=True)\n",
    "\n",
    "    # Extract text from the <p> element\n",
    "    if paragraph_element is not None:\n",
    "        paragraph_text = paragraph_element.get_text(strip=True)\n",
    "    else:\n",
    "        paragraph_text = None\n",
    "    \n",
    "    return job_title, company_name,salary,location_span, paragraph_text,date,job_type_span, ref\n",
    "        \n",
    "\n",
    "def get_infos(data_link):\n",
    "\n",
    "    # Initialize lists to store individual pieces of information\n",
    "    job_titles = []\n",
    "    company_names = []\n",
    "    job_types = []\n",
    "    salaries = []\n",
    "    locations = []\n",
    "    job_descriptions = []\n",
    "    Date=[]\n",
    "    ID= []\n",
    "\n",
    "    # Iterate through each URL in the provided list\n",
    "    for url in tqdm(data_link, desc='processing link'):\n",
    "        # Call the get_job function to extract information from the current URL\n",
    "        infos = get_job(url)\n",
    "\n",
    "        # Append the extracted information to the respective lists\n",
    "        job_titles.append(infos[0])\n",
    "        company_names.append(infos[1])\n",
    "        job_types.append(infos[6])\n",
    "        salaries.append(infos[2])\n",
    "        locations.append(infos[3])\n",
    "        job_descriptions.append(infos[4])\n",
    "        Date.append(infos[5])\n",
    "        ID.append(infos[7])\n",
    "        \n",
    "    # Create a dictionary with the collected information\n",
    "    data = {\n",
    "        'Job ID':ID,\n",
    "        'Job Title': job_titles,\n",
    "        'Company Name': company_names,\n",
    "        'Job Type': job_types,\n",
    "        'Date':Date,\n",
    "        'Salary': salaries,\n",
    "        'Location': locations,\n",
    "        'Job Description': job_descriptions,\n",
    "        'lien':link_data\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary into a DataFrame using pandas\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Return the DataFrame\n",
    "    return df\n",
    "\n",
    "df= get_infos(link_data)\n",
    "\n",
    "df.to_csv(r'C:\\Users\\komla\\Desktop\\Projet_Scraping\\hellowork_data.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b3a284-4a99-462c-ab39-b851436a81db",
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

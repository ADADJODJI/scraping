{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b327677-6c0e-4a60-846f-aaf4dab53785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.service import Service as ChromeService \n",
    "from webdriver_manager.chrome import ChromeDriverManager \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url= \"https://www.hellowork.com/fr-fr/\"\n",
    "\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to the page\n",
    "driver.get(url)\n",
    "\n",
    "# Select the search bar\n",
    "search_input = driver.find_element(by=By.NAME,value=\"k\")\n",
    "\n",
    "# Search for a keyword\n",
    "search_input.send_keys(\"data\")\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "# Press Enter\n",
    "search_input.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for some time\n",
    "time.sleep(5)\n",
    "\n",
    "# Define XPath expression for the \"Continue without accepting\" button\n",
    "continue_button_xpath = '//button[@id=\"hw-cc-notice-continue-without-accepting-btn\"]'\n",
    "\n",
    "continue_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, continue_button_xpath))\n",
    "    )\n",
    "\n",
    "# Click on the \"Continue without accepting\" button\n",
    "continue_button.click()\n",
    "\n",
    "# Define XPath expression for the \"li\" element\n",
    "xpath_expression = '//li[@class=\"next\"]'\n",
    "\n",
    "# Wait for the element to be clickable\n",
    "element = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.XPATH, xpath_expression))\n",
    ")\n",
    "\n",
    "# Click on the element\n",
    "element.click()\n",
    "time.sleep(5)\n",
    "\n",
    "def page_link():\n",
    "\n",
    "    # To store the links\n",
    "    link= []\n",
    "\n",
    "    css_expression = '.offer--content .offer--maininfo h3 a'\n",
    "\n",
    "    # Wait for elements to be present before retrieving them\n",
    "    job_links = WebDriverWait(driver, 1).until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, css_expression))\n",
    "    )\n",
    "    \n",
    "    for job_link in job_links:\n",
    "        offer_link = job_link.get_attribute('href')\n",
    "        link.append(offer_link)\n",
    "    \n",
    "    return link\n",
    "\n",
    "def change_page():\n",
    "\n",
    "    xpath_expression = '//li[@class=\"next\"]'\n",
    "\n",
    "    element = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, xpath_expression))\n",
    "    )\n",
    "\n",
    "    element.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "def get_all_link(nb_page):\n",
    "\n",
    "    link=[]\n",
    "\n",
    "    for _ in tqdm(range(1,nb_page), desc='Getting link on page'):\n",
    "\n",
    "        link.extend(page_link())\n",
    "        # Move to the next page\n",
    "        change_page()\n",
    "    \n",
    "    return link\n",
    "\n",
    "link_data = get_all_link(50)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def get_job(url):\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Use the find method to select the desired span element\n",
    "\n",
    "    # Job title\n",
    "    job_title_span = soup.find('span', {'class': 'tw-block tw-typo-xl sm:tw-typo-3xl tw-mb-2', 'data-cy': 'jobTitle'})\n",
    "    if job_title_span is not None:\n",
    "        job_title = job_title_span.text.strip()\n",
    "    else:\n",
    "        job_title = None\n",
    "\n",
    "    # Company name\n",
    "    company_name_span = soup.find('span', {'class': 'tw-contents tw-typo-m tw-text-grey'})\n",
    "    if company_name_span is not None:\n",
    "        company_name = company_name_span.text.strip()\n",
    "    else:\n",
    "        company_name = None\n",
    "\n",
    "    # Location and job type\n",
    "    spans = soup.find_all('span', {'class': 'tw-inline-flex tw-typo-m tw-text-grey'})\n",
    "    if len(spans) >= 2:\n",
    "        job_type_span = spans[1].text.strip()\n",
    "        location_span = spans[0].text.strip()\n",
    "    else:\n",
    "        job_type_span = None\n",
    "        location_span = None\n",
    "\n",
    "    # Salary\n",
    "    salary_span = soup.find('li', {'class': 'tw-tag-attractive-s tw-readonly'})\n",
    "    if salary_span is not None:\n",
    "        salary = salary_span.text.strip().replace('\\u202f', '')\n",
    "    else:\n",
    "        salary = None\n",
    "\n",
    "    # Publication date\n",
    "    date_span = soup.find('span', {'class': 'tw-block tw-typo-xs tw-text-grey tw-mt-3 tw-break-words'})\n",
    "    if date_span is not None:\n",
    "        date_text = date_span.text.strip().split(' ')\n",
    "        date = date_text[2]\n",
    "        ref = date_text[6].split('/')[0]\n",
    "    else:\n",
    "        date = None\n",
    "        ref = None\n",
    "\n",
    "    # Advertisement reference\n",
    "    ref_span = soup.find('span', {'class': 'tw-block tw-typo-xs tw-text-grey tw-mt-3 tw-break-words'})\n",
    "    if ref_span is not None:\n",
    "        ref_text = ref_span.text.strip().split(' ')\n",
    "        ref = ref_text[6].split('/')[0]\n",
    "    else:\n",
    "        ref = None\n",
    "\n",
    "    # Find the <p> element by its class name\n",
    "    paragraph_element = soup.find('p', class_='tw-typo-long-m')\n",
    "\n",
    "    # Extract text from the <p> element\n",
    "    paragraph_text = paragraph_element.get_text(strip=True)\n",
    "\n",
    "    # Extract text from the <p> element\n",
    "    if paragraph_element is not None:\n",
    "        paragraph_text = paragraph_element.get_text(strip=True)\n",
    "    else:\n",
    "        paragraph_text = None\n",
    "    \n",
    "    return job_title, company_name,salary,location_span, paragraph_text,date,job_type_span, ref\n",
    "        \n",
    "\n",
    "def get_infos(data_link):\n",
    "\n",
    "    # Initialize lists to store individual pieces of information\n",
    "    job_titles = []\n",
    "    company_names = []\n",
    "    job_types = []\n",
    "    salaries = []\n",
    "    locations = []\n",
    "    job_descriptions = []\n",
    "    Date=[]\n",
    "    ID= []\n",
    "\n",
    "    # Iterate through each URL in the provided list\n",
    "    for url in tqdm(data_link, desc='processing link'):\n",
    "        # Call the get_job function to extract information from the current URL\n",
    "        infos = get_job(url)\n",
    "\n",
    "        # Append the extracted information to the respective lists\n",
    "        job_titles.append(infos[0])\n",
    "        company_names.append(infos[1])\n",
    "        job_types.append(infos[6])\n",
    "        salaries.append(infos[2])\n",
    "        locations.append(infos[3])\n",
    "        job_descriptions.append(infos[4])\n",
    "        Date.append(infos[5])\n",
    "        ID.append(infos[7])\n",
    "        \n",
    "    # Create a dictionary with the collected information\n",
    "    data = {\n",
    "        'Job ID':ID,\n",
    "        'Job Title': job_titles,\n",
    "        'Company Name': company_names,\n",
    "        'Job Type': job_types,\n",
    "        'Date':Date,\n",
    "        'Salary': salaries,\n",
    "        'Location': locations,\n",
    "        'Job Description': job_descriptions,\n",
    "        'lien':link_data\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary into a DataFrame using pandas\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Return the DataFrame\n",
    "    return df\n",
    "\n",
    "df= get_infos(link_data)\n",
    "\n",
    "df.to_csv(r'C:\\Users\\komla\\Desktop\\Projet_Scraping\\hellowork_data.csv', index=False, encoding='utf-8')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

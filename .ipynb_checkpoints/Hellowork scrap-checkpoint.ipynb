{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e89d464d-536e-4066-aa6d-8aae84cbe092",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.service import Service as ChromeService \n",
    "from webdriver_manager.chrome import ChromeDriverManager \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys # le button entrer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d52b5542-1fa5-40c5-a38b-3131403695b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting link on page: 100%|██████████| 27/27 [01:02<00:00,  2.33s/it]\n",
      "processing link: 100%|██████████| 540/540 [08:12<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.service import Service as ChromeService \n",
    "from webdriver_manager.chrome import ChromeDriverManager \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys # le button entrer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "url= \"https://www.hellowork.com/fr-fr/\"\n",
    "\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# naviger sur la page\n",
    "driver.get(url)\n",
    "\n",
    "# Sélectionner la barre de recherche\n",
    "search_input = driver.find_element(by=By.NAME,value=\"k\")\n",
    "\n",
    "# element de recherche\n",
    "search_input.send_keys(\"data\")\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "# Button entrer\n",
    "search_input.send_keys(Keys.RETURN)\n",
    "\n",
    "# temps d'attente\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Définir l'expression XPath pour le bouton \"Continuer sans accepter\"\n",
    "continue_button_xpath = '//button[@id=\"hw-cc-notice-continue-without-accepting-btn\"]'\n",
    "\n",
    "continue_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, continue_button_xpath))\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    # Cliquer sur le bouton \"Continuer sans accepter\"\n",
    "continue_button.click()\n",
    "\n",
    "# Définir l'expression XPath pour l'élément <li>\n",
    "\n",
    "xpath_expression = '//li[@class=\"next\"]'\n",
    "\n",
    "# Attendre que l'élément soit cliquable\n",
    "element = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.XPATH, xpath_expression))\n",
    ")\n",
    "\n",
    "# Cliquer sur l'élément\n",
    "element.click()\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "\n",
    "def page_link():\n",
    "\n",
    "#\"\"\"avoir les lins sur une page\"\"\"\n",
    "\n",
    "    # pour stoker les lien\n",
    "    link= []\n",
    "        # Définir l'expression XPath\n",
    "    css_expression = '.offer--content .offer--maininfo h3 a'\n",
    "\n",
    "    # Attendre la présence des éléments avant de les récupérer\n",
    "    job_links = WebDriverWait(driver, 1).until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, css_expression))\n",
    "    )\n",
    "    # Utiliser les liens d'emploi récupérés\n",
    "    for job_link in job_links:\n",
    "        lien_offre = job_link.get_attribute('href')\n",
    "        link.append(lien_offre)\n",
    "    \n",
    "    return link\n",
    "\n",
    "def change_page():\n",
    "    # Définir l'expression XPath pour l'élément <li>\n",
    "\n",
    "    xpath_expression = '//li[@class=\"next\"]'\n",
    "\n",
    "    # Attendre que l'élément soit cliquable\n",
    "    element = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, xpath_expression))\n",
    "    )\n",
    "\n",
    "    # Cliquer sur l'élément\n",
    "    element.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "def get_all_link(nb_page):\n",
    "\n",
    "    link=[]\n",
    "\n",
    "    for _ in tqdm(range(1,nb_page), desc='Getting link on page'):\n",
    "\n",
    "        link.extend(page_link())\n",
    "        # next page\n",
    "        change_page()\n",
    "    \n",
    "    return link\n",
    "\n",
    "# Fermez le navigateur après avoir terminé\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "# Get link data  specify the number of page to get\n",
    "link_data = get_all_link(28)\n",
    "#########################################################################\n",
    "driver.quit()\n",
    "\n",
    "# sauvegarder les lien\n",
    "#df = pd.DataFrame({\n",
    "#    \"Lien\":link_data})\n",
    "\n",
    "#df.to_csv(r'C:\\Users\\33753\\Desktop\\PYLAB\\Algorithm\\hellowork_data1.csv', index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def get_job(url):\n",
    "\n",
    "    # Fetch the HTML content from the URL with headers\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Utilisez la méthode find pour sélectionner l'élément span désiré\n",
    "\n",
    "    # Titre du poste\n",
    "    job_title_span = soup.find('span', {'class': 'tw-block tw-typo-xl sm:tw-typo-3xl tw-mb-2', 'data-cy': 'jobTitle'})\n",
    "    if job_title_span is not None:\n",
    "        job_title = job_title_span.text.strip()\n",
    "    else:\n",
    "        job_title = None\n",
    "\n",
    "    # Nom de l'entreprise\n",
    "    company_name_span = soup.find('span', {'class': 'tw-contents tw-typo-m tw-text-grey'})\n",
    "    if company_name_span is not None:\n",
    "        company_name = company_name_span.text.strip()\n",
    "    else:\n",
    "        company_name = None\n",
    "\n",
    "    # Emplacement et type de poste\n",
    "    spans = soup.find_all('span', {'class': 'tw-inline-flex tw-typo-m tw-text-grey'})\n",
    "    if len(spans) >= 2:\n",
    "        job_type_span = spans[1].text.strip()\n",
    "        location_span = spans[0].text.strip()\n",
    "    else:\n",
    "        job_type_span = None\n",
    "        location_span = None\n",
    "\n",
    "    # Salaire\n",
    "    salary_span = soup.find('li', {'class': 'tw-tag-attractive-s tw-readonly'})\n",
    "    if salary_span is not None:\n",
    "        salary = salary_span.text.strip().replace('\\u202f', '')\n",
    "    else:\n",
    "        salary = None\n",
    "\n",
    "    # Date de publication\n",
    "    date_span = soup.find('span', {'class': 'tw-block tw-typo-xs tw-text-grey tw-mt-3 tw-break-words'})\n",
    "    if date_span is not None:\n",
    "        date_text = date_span.text.strip().split(' ')\n",
    "        date = date_text[2]\n",
    "        ref = date_text[6].split('/')[0]\n",
    "    else:\n",
    "        date = None\n",
    "        ref = None\n",
    "\n",
    "    # Référence de l'annonce\n",
    "    ref_span = soup.find('span', {'class': 'tw-block tw-typo-xs tw-text-grey tw-mt-3 tw-break-words'})\n",
    "    if ref_span is not None:\n",
    "        ref_text = ref_span.text.strip().split(' ')\n",
    "        ref = ref_text[6].split('/')[0]\n",
    "    else:\n",
    "        ref = None\n",
    "\n",
    "    # Trouver l'élément <p> par son nom de classe\n",
    "    paragraph_element = soup.find('p', class_='tw-typo-long-m')\n",
    "\n",
    "    # Extraire le texte de l'élément <p>\n",
    "    paragraph_text = paragraph_element.get_text(strip=True)\n",
    "\n",
    "    # Extraire le texte de l'élément <p>\n",
    "    if paragraph_element is not None:\n",
    "        paragraph_text = paragraph_element.get_text(strip=True)\n",
    "    else:\n",
    "        paragraph_text = None\n",
    "    \n",
    "    return job_title, company_name,salary,location_span, paragraph_text,date,job_type_span, ref\n",
    "        \n",
    "\n",
    "\n",
    "def get_infos(data_link):\n",
    "    # Initialize lists to store individual pieces of information\n",
    "    job_titles = []\n",
    "    company_names = []\n",
    "    job_types = []\n",
    "    salaries = []\n",
    "    locations = []\n",
    "    job_descriptions = []\n",
    "    Date=[]\n",
    "    ID= []\n",
    "\n",
    "    # Iterate through each URL in the provided list\n",
    "    for url in tqdm(data_link, desc='processing link'):\n",
    "        # Call the get_job function to extract information from the current URL\n",
    "        infos = get_job(url)\n",
    "\n",
    "        # Append the extracted information to the respective lists\n",
    "        job_titles.append(infos[0])\n",
    "        company_names.append(infos[1])\n",
    "        job_types.append(infos[6])\n",
    "        salaries.append(infos[2])\n",
    "        locations.append(infos[3])\n",
    "        job_descriptions.append(infos[4])\n",
    "        Date.append(infos[5])\n",
    "        ID.append(infos[7])\n",
    "        \n",
    "\n",
    "    # Create a dictionary with the collected information\n",
    "    data = {\n",
    "        'Job ID':ID,\n",
    "        'Job Title': job_titles,\n",
    "        'Company Name': company_names,\n",
    "        'Job Type': job_types,\n",
    "        'Date':Date,\n",
    "        'Salary': salaries,\n",
    "        'Location': locations,\n",
    "        'Job Description': job_descriptions,\n",
    "        'lien':link_data\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary into a DataFrame using pandas\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Return the DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "df= get_infos(link_data)\n",
    "\n",
    "df.to_csv(r'C:\\Users\\komla\\Desktop\\Projet_Scraping\\hellowork_data7.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8eb54051-771f-4598-8a58-db196229c72c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting link on page: 100%|██████████| 49/49 [01:59<00:00,  2.43s/it]\n",
      "processing link: 100%|██████████| 980/980 [18:45<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.service import Service as ChromeService \n",
    "from webdriver_manager.chrome import ChromeDriverManager \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url= \"https://www.hellowork.com/fr-fr/\"\n",
    "\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to the page\n",
    "driver.get(url)\n",
    "\n",
    "# Select the search bar\n",
    "search_input = driver.find_element(by=By.NAME,value=\"k\")\n",
    "\n",
    "# Search for a keyword\n",
    "search_input.send_keys(\"data\")\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "# Press Enter\n",
    "search_input.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for some time\n",
    "time.sleep(5)\n",
    "\n",
    "# Define XPath expression for the \"Continue without accepting\" button\n",
    "continue_button_xpath = '//button[@id=\"hw-cc-notice-continue-without-accepting-btn\"]'\n",
    "\n",
    "continue_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, continue_button_xpath))\n",
    "    )\n",
    "\n",
    "# Click on the \"Continue without accepting\" button\n",
    "continue_button.click()\n",
    "\n",
    "# Define XPath expression for the \"li\" element\n",
    "xpath_expression = '//li[@class=\"next\"]'\n",
    "\n",
    "# Wait for the element to be clickable\n",
    "element = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.XPATH, xpath_expression))\n",
    ")\n",
    "\n",
    "# Click on the element\n",
    "element.click()\n",
    "time.sleep(5)\n",
    "\n",
    "def page_link():\n",
    "\n",
    "    # To store the links\n",
    "    link= []\n",
    "\n",
    "    css_expression = '.offer--content .offer--maininfo h3 a'\n",
    "\n",
    "    # Wait for elements to be present before retrieving them\n",
    "    job_links = WebDriverWait(driver, 1).until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, css_expression))\n",
    "    )\n",
    "    \n",
    "    for job_link in job_links:\n",
    "        offer_link = job_link.get_attribute('href')\n",
    "        link.append(offer_link)\n",
    "    \n",
    "    return link\n",
    "\n",
    "def change_page():\n",
    "\n",
    "    xpath_expression = '//li[@class=\"next\"]'\n",
    "\n",
    "    element = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, xpath_expression))\n",
    "    )\n",
    "\n",
    "    element.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "def get_all_link(nb_page):\n",
    "\n",
    "    link=[]\n",
    "\n",
    "    for _ in tqdm(range(1,nb_page), desc='Getting link on page'):\n",
    "\n",
    "        link.extend(page_link())\n",
    "        # Move to the next page\n",
    "        change_page()\n",
    "    \n",
    "    return link\n",
    "\n",
    "link_data = get_all_link(50)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def get_job(url):\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Use the find method to select the desired span element\n",
    "\n",
    "    # Job title\n",
    "    job_title_span = soup.find('span', {'class': 'tw-block tw-typo-xl sm:tw-typo-3xl tw-mb-2', 'data-cy': 'jobTitle'})\n",
    "    if job_title_span is not None:\n",
    "        job_title = job_title_span.text.strip()\n",
    "    else:\n",
    "        job_title = None\n",
    "\n",
    "    # Company name\n",
    "    company_name_span = soup.find('span', {'class': 'tw-contents tw-typo-m tw-text-grey'})\n",
    "    if company_name_span is not None:\n",
    "        company_name = company_name_span.text.strip()\n",
    "    else:\n",
    "        company_name = None\n",
    "\n",
    "    # Location and job type\n",
    "    spans = soup.find_all('span', {'class': 'tw-inline-flex tw-typo-m tw-text-grey'})\n",
    "    if len(spans) >= 2:\n",
    "        job_type_span = spans[1].text.strip()\n",
    "        location_span = spans[0].text.strip()\n",
    "    else:\n",
    "        job_type_span = None\n",
    "        location_span = None\n",
    "\n",
    "    # Salary\n",
    "    salary_span = soup.find('li', {'class': 'tw-tag-attractive-s tw-readonly'})\n",
    "    if salary_span is not None:\n",
    "        salary = salary_span.text.strip().replace('\\u202f', '')\n",
    "    else:\n",
    "        salary = None\n",
    "\n",
    "    # Publication date\n",
    "    date_span = soup.find('span', {'class': 'tw-block tw-typo-xs tw-text-grey tw-mt-3 tw-break-words'})\n",
    "    if date_span is not None:\n",
    "        date_text = date_span.text.strip().split(' ')\n",
    "        date = date_text[2]\n",
    "        ref = date_text[6].split('/')[0]\n",
    "    else:\n",
    "        date = None\n",
    "        ref = None\n",
    "\n",
    "    # Advertisement reference\n",
    "    ref_span = soup.find('span', {'class': 'tw-block tw-typo-xs tw-text-grey tw-mt-3 tw-break-words'})\n",
    "    if ref_span is not None:\n",
    "        ref_text = ref_span.text.strip().split(' ')\n",
    "        ref = ref_text[6].split('/')[0]\n",
    "    else:\n",
    "        ref = None\n",
    "\n",
    "    # Find the <p> element by its class name\n",
    "    paragraph_element = soup.find('p', class_='tw-typo-long-m')\n",
    "\n",
    "    # Extract text from the <p> element\n",
    "    paragraph_text = paragraph_element.get_text(strip=True)\n",
    "\n",
    "    # Extract text from the <p> element\n",
    "    if paragraph_element is not None:\n",
    "        paragraph_text = paragraph_element.get_text(strip=True)\n",
    "    else:\n",
    "        paragraph_text = None\n",
    "    \n",
    "    return job_title, company_name,salary,location_span, paragraph_text,date,job_type_span, ref\n",
    "        \n",
    "\n",
    "def get_infos(data_link):\n",
    "\n",
    "    # Initialize lists to store individual pieces of information\n",
    "    job_titles = []\n",
    "    company_names = []\n",
    "    job_types = []\n",
    "    salaries = []\n",
    "    locations = []\n",
    "    job_descriptions = []\n",
    "    Date=[]\n",
    "    ID= []\n",
    "\n",
    "    # Iterate through each URL in the provided list\n",
    "    for url in tqdm(data_link, desc='processing link'):\n",
    "        # Call the get_job function to extract information from the current URL\n",
    "        infos = get_job(url)\n",
    "\n",
    "        # Append the extracted information to the respective lists\n",
    "        job_titles.append(infos[0])\n",
    "        company_names.append(infos[1])\n",
    "        job_types.append(infos[6])\n",
    "        salaries.append(infos[2])\n",
    "        locations.append(infos[3])\n",
    "        job_descriptions.append(infos[4])\n",
    "        Date.append(infos[5])\n",
    "        ID.append(infos[7])\n",
    "        \n",
    "    # Create a dictionary with the collected information\n",
    "    data = {\n",
    "        'Job ID':ID,\n",
    "        'Job Title': job_titles,\n",
    "        'Company Name': company_names,\n",
    "        'Job Type': job_types,\n",
    "        'Date':Date,\n",
    "        'Salary': salaries,\n",
    "        'Location': locations,\n",
    "        'Job Description': job_descriptions,\n",
    "        'lien':link_data\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary into a DataFrame using pandas\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Return the DataFrame\n",
    "    return df\n",
    "\n",
    "df= get_infos(link_data)\n",
    "\n",
    "df.to_csv(r'C:\\Users\\komla\\Desktop\\Projet_Scraping\\hellowork_data8.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b3a284-4a99-462c-ab39-b851436a81db",
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
